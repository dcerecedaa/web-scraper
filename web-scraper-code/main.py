import sys
import logging
from datetime import datetime
from colorama import Fore, Style, init
from tqdm import tqdm
from scraper.fetcher import get_page, close_fetcher
from scraper.parser import UniversalParser
from scraper.storage import save_csv
from scraper.config import MAX_PRODUCTS_PER_CATEGORY, MAX_CATEGORIES

# Inicializar colorama para colores en terminal
init(autoreset=True)

# Configurar logging
def setup_logging():
    """Configura el sistema de logging"""
    log_dir = 'logs'
    import os
    os.makedirs(log_dir, exist_ok=True)
    
    log_filename = f'logs/scraper_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return log_filename

def print_banner():
    """Imprime el banner del scraper"""
    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}üõçÔ∏è  SCRAPER UNIVERSAL DE PRODUCTOS")
    print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}\n")

def get_user_url():
    """Solicita la URL al usuario"""
    print(f"{Fore.YELLOW}üìå Introduce la URL de la tienda que quieres scrapear:{Style.RESET_ALL}")
    print(f"{Fore.WHITE}   Ejemplo: https://www2.hm.com/")
    print(f"{Fore.WHITE}   Ejemplo: https://www.zara.com/es/\n")
    
    url = input(f"{Fore.GREEN}‚ûú URL: {Style.RESET_ALL}").strip()
    
    if not url.startswith('http'):
        url = 'https://' + url
    
    return url

def scrape_category(parser, category_info, all_products):
    """Scrape una categor√≠a espec√≠fica"""
    genero = category_info['genero']
    categoria = category_info['categoria']
    url = category_info['url']
    
    print(f"\n{Fore.BLUE}üìÇ Scrapeando: {genero} ‚Üí {categoria}{Style.RESET_ALL}")
    
    try:
        html = get_page(url)
        if not html:
            return
        
        products = parser.parse_products(html)
        
        # A√±adir g√©nero y categor√≠a a cada producto
        for product in products:
            product['genero'] = genero
            product['categoria'] = categoria
        
        all_products.extend(products)
        print(f"{Fore.GREEN}‚úÖ {len(products)} productos encontrados{Style.RESET_ALL}")
        
    except Exception as e:
        print(f"{Fore.RED}‚ùå Error en {categoria}: {e}{Style.RESET_ALL}")
        logging.error(f"Error scrapeando {url}: {e}")

def run_scraper():
    """Ejecuta el scraper completo"""
    log_file = setup_logging()
    print_banner()
    
    # Obtener URL del usuario
    base_url = get_user_url()
    
    print(f"\n{Fore.CYAN}üöÄ Iniciando scraper para: {base_url}{Style.RESET_ALL}")
    print(f"{Fore.WHITE}üìù Log guardado en: {log_file}{Style.RESET_ALL}\n")
    
    all_products = []
    
    try:
        # Inicializar parser
        parser = UniversalParser(base_url)
        
        # Obtener p√°gina principal
        print(f"{Fore.YELLOW}üîç Analizando p√°gina principal...{Style.RESET_ALL}")
        home_html = get_page(base_url)
        
        if not home_html:
            print(f"{Fore.RED}‚ùå No se pudo cargar la p√°gina principal{Style.RESET_ALL}")
            return
        
        # Encontrar todas las categor√≠as
        categories = parser.find_categories(home_html)
        
        if not categories:
            print(f"{Fore.YELLOW}‚ö†Ô∏è  No se encontraron categor√≠as autom√°ticamente{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}   Intentando scrapear la p√°gina principal...{Style.RESET_ALL}")
            
            products = parser.parse_products(home_html)
            for product in products:
                product['genero'] = 'Sin clasificar'
                product['categoria'] = 'General'
            all_products.extend(products)
        else:
            print(f"\n{Fore.GREEN}‚úÖ {len(categories)} categor√≠as detectadas{Style.RESET_ALL}")
            
            # Limitar categor√≠as si hay demasiadas
            if len(categories) > MAX_CATEGORIES:
                print(f"{Fore.YELLOW}‚ö†Ô∏è  Limitando a {MAX_CATEGORIES} categor√≠as{Style.RESET_ALL}")
                categories = categories[:MAX_CATEGORIES]
            
            # Mostrar resumen de categor√≠as
            print(f"\n{Fore.CYAN}üìã Categor√≠as a scrapear:{Style.RESET_ALL}")
            generos = {}
            for cat in categories:
                genero = cat['genero']
                if genero not in generos:
                    generos[genero] = []
                generos[genero].append(cat['categoria'])
            
            for genero, cats in generos.items():
                print(f"  ‚Ä¢ {genero}: {', '.join(set(cats))}")
            
            # Preguntar si continuar
            print(f"\n{Fore.YELLOW}¬øContinuar con el scraping? (s/n): {Style.RESET_ALL}", end='')
            response = input().strip().lower()
            
            if response != 's':
                print(f"{Fore.RED}‚ùå Scraping cancelado{Style.RESET_ALL}")
                return
            
            # Scrapear cada categor√≠a
            for category_info in tqdm(categories, desc="Progreso total", colour='green'):
                scrape_category(parser, category_info, all_products)
                
                # Limitar productos por categor√≠a
                if len(all_products) > MAX_PRODUCTS_PER_CATEGORY * len(categories):
                    print(f"\n{Fore.YELLOW}‚ö†Ô∏è  L√≠mite de productos alcanzado{Style.RESET_ALL}")
                    break
        
        # Guardar resultados
        print(f"\n{Fore.CYAN}üíæ Guardando resultados...{Style.RESET_ALL}")
        
        if not all_products:
            print(f"{Fore.RED}‚ùå No se encontraron productos{Style.RESET_ALL}")
            return
        
        brand_name = parser.brand_config.get('name', parser.domain) if parser.brand_config else parser.domain
        filepath = save_csv(all_products, brand_name.replace(' ', '_'))
        
        # Resumen final
        print(f"\n{Fore.GREEN}{'='*60}")
        print(f"‚úÖ SCRAPING COMPLETADO")
        print(f"{'='*60}{Style.RESET_ALL}")
        print(f"\n{Fore.CYAN}üìä Resumen:{Style.RESET_ALL}")
        print(f"  ‚Ä¢ Total productos: {Fore.GREEN}{len(all_products)}{Style.RESET_ALL}")
        print(f"  ‚Ä¢ Archivo principal: {Fore.BLUE}{filepath}{Style.RESET_ALL}")
        print(f"  ‚Ä¢ Log: {Fore.BLUE}{log_file}{Style.RESET_ALL}")
        
        # Desglose por categor√≠a
        from collections import Counter
        categorias_count = Counter([p.get('categoria', 'Sin categor√≠a') for p in all_products])
        print(f"\n{Fore.CYAN}üìÇ Por categor√≠a:{Style.RESET_ALL}")
        for cat, count in categorias_count.most_common():
            print(f"  ‚Ä¢ {cat}: {count}")
        
    except KeyboardInterrupt:
        print(f"\n\n{Fore.RED}‚ö†Ô∏è  Scraping interrumpido por el usuario{Style.RESET_ALL}")
    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Error fatal: {e}{Style.RESET_ALL}")
        logging.error(f"Error fatal: {e}", exc_info=True)
    finally:
        # Cerrar navegador
        print(f"\n{Fore.YELLOW}üîí Cerrando navegador...{Style.RESET_ALL}")
        close_fetcher()
        print(f"{Fore.GREEN}‚úÖ Proceso finalizado{Style.RESET_ALL}\n")

if __name__ == "__main__":
    run_scraper()